# nlp_paper

## 1. Deep Learning from scrach 2_sub study

### paper1 (05/28/20)
- [Efficient Estimation of Word Representations in Vector Space (original word2vec paper)](https://arxiv.org/pdf/1301.3781.pdf)
### paper2 (06/04/20)
- [Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper)](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
### paper3 (06/11/20)
- [Understanding LSTM Networks (blog post overview)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [The Unreasonable Effectiveness of Recurrent Neural Networks (blog post overview)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
### paper4 (06/18/20, cancel)
- [Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)](https://arxiv.org/pdf/1409.3215.pdf)
### paper5 (06/25/20)
- [Neural Machine Translation by Jointly Learning to Align and Translate (original seq2seq+attention paper)](https://arxiv.org/pdf/1409.0473.pdf)
### paper6 in main study (06/30/20)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
### paper7 in main study (07/07/20)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
### paper8 (07/10/20)
- [Deep contextualized word representations(ELMO)](https://arxiv.org/pdf/1802.05365.pdf)


## 2. beyondBERT

### week02 (06/20/20)
- [The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives](https://arxiv.org/abs/1909.01380)
- [How multilingual is Multilingual BERT?](https://arxiv.org/abs/1906.01502)
### week03 (06/27/20)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)
### week04 (07/04/20)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- [Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/abs/2003.02245)
### week05 (07/11/20)
- [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
### week06 (07/18/20)
- [Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://arxiv.org/abs/1904.09324)
- [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848)
### week07 (07/25/20)
- [You Impress Me: Dialogue Generation via Mutual Persona Perception](https://arxiv.org/abs/2004.05388)
- [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637)
### week08 (08/01/20)
- [ToD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogues](https://arxiv.org/abs/2004.06871)
- [A Simple Language Model for Task-Oriented Dialogue](https://arxiv.org/abs/2005.00796)
### week09 (08/08/20)
- [ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation](https://arxiv.org/abs/1907.05339)
- [FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/abs/2004.02178)
### week10 (08/22/20)
- [PoWER-BERT: Accelerating BERT inference for Classification Tasks](https://arxiv.org/abs/2001.08950)
- [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
### week11 (08/29/20)
- [GPT3: Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)


## 3. Model Implematation (Code Review)

### week0 (07/20/20) 
 - Planning
 
### week1 (08/10/20) 
 - transformer: architecture
 
### week2 (08/21/20)
- transformer: label smoothing/beam search

### week3 (08/28/20)
- transformer: trainning/multi-GPU/experiment
